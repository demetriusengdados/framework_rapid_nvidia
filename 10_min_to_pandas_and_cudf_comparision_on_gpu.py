# -*- coding: utf-8 -*-
"""10 Min to Pandas and Cudf Comparision on GPU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ashishpatel26/Rapidsai_Machine_learning_on_GPU/blob/main/10_Min_to_Pandas_and_Cudf_Comparision_on_GPU.ipynb

![](https://github.com/ashishpatel26/Rapidsai_Machine_learning_on_GPU/raw/main/images/rapidsailogo.jpg?raw=true)

ðŸ¤©**About Rapids**ðŸ¤©

- The RAPIDS suite of open source software libraries aim to enable execution of end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIAÂ® CUDAÂ® primitives for low-level compute optimization, but exposing that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.
![](https://github.com/rapidsai/cudf/raw/branch-21.08/img/rapids_arrow.png)

### **What are these Libraries?**
* [cuDF](https://github.com/rapidsai/cudf) is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating tabular data using a DataFrame style API.

* [Dask](https://dask.org/) is a flexible library for parallel computing in Python that makes scaling out your workflow smooth and simple. On the CPU, Dask uses Pandas to execute operations in parallel on DataFrame partitions.

* [Dask-cuDF](https://github.com/rapidsai/cudf/tree/main/python/dask_cudfhttps://github.com/rapidsai/cudf/tree/main/python/dask_cudf) extends Dask where necessary to allow its DataFrame partitions to be processed by cuDF GPU DataFrames as opposed to Pandas DataFrames. For instance, when you call dask_cudf.read_csv(â€¦), your clusterâ€™s GPUs do the work of parsing the CSV file(s) with underlying cudf.read_csv().

### **When to use cuDF and Dask-cuDF**
* If your workflow is fast enough on a single GPU or your data comfortably fits in memory on a single GPU, you would want to use cuDF. If you want to distribute your workflow across multiple GPUs, have more data than you can fit in memory on a single GPU, or want to analyze data spread across many files at once, you would want to use Dask-cuDF.

# 10 Min to Pandas and cuDF Comparision on GPU

ðŸŽ¯ Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.

ðŸŽ¯ cuDF provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.

### **1.Installation of cuDF**

> ðŸ¹**Conda installation** - ðŸ“¥Recommended

```console
# for CUDA 11.0
conda install -c rapidsai -c nvidia -c numba -c conda-forge \
    cudf=21.06 python=3.7 cudatoolkit=11.0

# or, for CUDA 11.2
conda install -c rapidsai -c nvidia -c numba -c conda-forge \
    cudf=21.06 python=3.7 cudatoolkit=11.2
```
> ðŸ¹**For Nightly Version**

```console
# for CUDA 11.0
conda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge \
    cudf python=3.7 cudatoolkit=11.0

# or, for CUDA 11.2
conda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge \
    cudf python=3.7 cudatoolkit=11.2
```
> ðŸ¹**PIP Installation**

```console
!pip install cudf
```

> ðŸ¹**Colab Installation** - [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&offline=true&sandboxMode=true
)
"""

# Install RAPIDS
!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
!bash rapidsai-csp-utils/colab/rapids-colab.sh stable

import sys, os

dist_package_index = sys.path.index('/usr/local/lib/python3.7/dist-packages')
sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.7/site-packages'] + sys.path[dist_package_index:]
sys.path
exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())

"""### **2.Import Libraries**"""

import os
import cupy as cp
import pandas as pd
import cudf
import dask_cudf
import time

cp.random.seed(2021)

print(cudf.__version__)
print(dask_cudf.__version__)
print(pd.__version__)

"""### **3.Object Creation**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 3.84ms| 9.6ms| 18.5ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# s = pd.Series([1,2,3,None, 4])
# print(s)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# s = cudf.Series([1,2,3,None, 4])
# print(s)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ds = dask_cudf.from_cudf(s, npartitions=2)
# print(ds.compute())

"""### **4.Loading Dataset**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 2.42s| 1.52s| 1.62s |

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# data = pd.read_csv("https://storage.googleapis.com/industryanalytics/LoanDefaultData.csv")
# display(data.head(2))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# data_cudf = cudf.read_csv("https://storage.googleapis.com/industryanalytics/LoanDefaultData.csv")
# display(data_cudf.head(2))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.dask_cudf
# data_daskcudf = dask_cudf.from_cudf(cudf.read_csv("https://storage.googleapis.com/industryanalytics/LoanDefaultData.csv"), npartitions=8)
# display(data_daskcudf.head(2))

"""### **5.Convert Pandas data to cudf and dask_cudf**

| Framework  | Pandas to cudf | Pandas to daskcudf |
| ---- | ---- | ---- | 
| Time | 2.73s| 2.84s| 
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. cudf from pandas
# data = pd.read_csv("https://storage.googleapis.com/industryanalytics/LoanDefaultData.csv")
# cudf_data = cudf.DataFrame.from_pandas(data)
# cudf_data

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. dask_cudf from pandas
# data = pd.read_csv("https://storage.googleapis.com/industryanalytics/LoanDefaultData.csv")
# cudf_data = cudf.DataFrame.from_pandas(data)
# daskcudf_data = dask_cudf.from_cudf(cudf_data, npartitions=2)

"""### **6.Viewing Data**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 241 Âµs| 4.32ms| 12.7ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# data.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# cudf_data.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# daskcudf_data.head()

"""### **7.Sorting Values**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 149ms| 37.8ms| 4.67s |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# data.sort_values('cust_id').head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# cudf_data.sort_values('cust_id').head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# daskcudf_data.sort_values('cust_id').head()

"""### **8.Selection**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 46.3Âµs| 1.18ms| 7.37ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# data["state"]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# cudf_data["state"]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3. cudf
# daskcudf_data["state"].compute()

"""### **9.Selection By label**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 1.4ms| 1.82ms| 7.45ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# data.loc[1:100, ["state","income_type"]]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# cudf_data.loc[1:100, ["state","income_type"]]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# daskcudf_data.loc[1:100, ["state","income_type"]].compute()

"""### **10.Selection by Position**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 1.4ms| 1.82ms| 30.1ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# data.iloc[4:10, 0:2].head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# cudf_data.iloc[4:10, 0:2].head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# daskcudf_data.compute().iloc[4:10, 0:2].head()

"""### **11.Boolean Indexing**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 182ms| 34.7ms| 9.8ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# data[data.cust_id>83000]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# cudf_data[cudf_data.cust_id>83000]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3. daskcudf
# daskcudf_data[daskcudf_data.cust_id>83000]

"""### **12.Query API**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 189ms| 178ms| 75.5ms |

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. pandas
# data.query("cust_id > 83000")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# cudf_data.query("cust_id > 83000")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3. daskcudf
# daskcudf_data.query("cust_id > 83000").compute()

"""### **13.`isin` Method**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 49.9ms| 44.2ms| 67.5ms |

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# data[data.state.isin(["Bihar", "Odisha"])]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# cudf_data[cudf_data.state.isin(["Bihar", "Odisha"])]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3. daskcudf
# daskcudf_data[daskcudf_data.state.isin(["Bihar", "Odisha"])].compute()

"""### **14.MultiIndex**

| Framework  | Pandas | cudf |
| ---- | ---- | ---- | 
| Time | 5.9ms| 12.9ms|
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# arrays = [['a', 'a', 'b', 'b'], [1, 2, 3, 4]]
# tuples = list(zip(*arrays))
# idx = pd.MultiIndex.from_tuples(tuples)
# display(idx)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# arrays = [['a', 'a', 'b', 'b'], [1, 2, 3, 4]]
# tuples = list(zip(*arrays))
# idx = cudf.MultiIndex.from_tuples(tuples)
# display(idx)

"""### **15.Missing Data**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 380ms| 29.8ms| 85.4ms |

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# data.fillna(999)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# cudf_data.fillna(999)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.cudf
# daskcudf_data.fillna(999).compute()

"""### **16.Statistics Operation**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 5.7ms| 4.51ms| 651ms |

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# data.total_pymnt.mean(), data.total_pymnt.var()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# cudf_data.total_pymnt.mean(), cudf_data.total_pymnt.var()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# daskcudf_data.total_pymnt.mean().compute(), daskcudf_data.total_pymnt.var().compute()

"""### **17.Applymap**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 211ms| 1.84ms| 7.16ms |

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# def add(n):
#   return n + 5
# data["total_pymnt"].apply(add)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# def add(n):
#   return n + 5
# cudf_data["total_pymnt"].applymap(add)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# def add(n):
#   return n + 5
# daskcudf_data["total_pymnt"].compute().applymap(add)

"""### **18. Histogramming**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 68.4ms| 23.5ms| 36.5ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# data["state"].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# cudf_data["state"].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3. daskcudf
# daskcudf_data["state"].value_counts().compute()

"""### **19.String Methods**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 3.59ms| 6.17ms| 16.4ms |
"""

states = data.state.unique().tolist()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# s = pd.Series(states)
# print(s.str.lower())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# s_cudf = cudf.Series(states)
# print(s_cudf.str.lower())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# s_daskcudf = dask_cudf.from_cudf(cudf.Series(states), npartitions=2)
# print(s_daskcudf.str.lower().compute())

"""### **20.Concat**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 1.41ms| 2.28ms| 11.1ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# pd.concat([s,s])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# cudf.concat([s_cudf,s_cudf])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3. daskcudf
# dask_cudf.concat([s_daskcudf,s_daskcudf]).compute()

"""### **21.Join**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 15.7ms| 9.05ms| 30.2ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# df_a = pd.DataFrame()
# df_a['key'] = ['a', 'b', 'c', 'd', 'e']
# df_a['vals_a'] = [float(i + 10) for i in range(5)]
# 
# df_b = pd.DataFrame()
# df_b['key'] = ['a', 'c', 'e']
# df_b['vals_b'] = [float(i+100) for i in range(3)]
# 
# merged = df_a.merge(df_b, on=['key'], how='left')
# merged.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# df_a = cudf.DataFrame()
# df_a['key'] = ['a', 'b', 'c', 'd', 'e']
# df_a['vals_a'] = [float(i + 10) for i in range(5)]
# 
# df_b = cudf.DataFrame()
# df_b['key'] = ['a', 'c', 'e']
# df_b['vals_b'] = [float(i+100) for i in range(3)]
# 
# merged = df_a.merge(df_b, on=['key'], how='left')
# merged.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# ddf_a = dask_cudf.from_cudf(df_a, npartitions=2)
# ddf_b = dask_cudf.from_cudf(df_b, npartitions=2)
# 
# merged = ddf_a.merge(ddf_b, on=['key'], how='left').compute()
# merged

"""### **22.Append**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 574Âµs| 4.26ms| 30.2ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. Pandas
# s.append(s)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. cudf
# s_cudf.append(s_cudf)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# s_daskcudf.append(s_daskcudf).compute()

"""### **23.Grouping**

| Framework  | Pandas | cudf |
| ---- | ---- | ---- | 
| Time | 792ms| 270ms| 
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# data["agg_col1"] = [1 if x % 2 == 0 else 0 for x in range(len(data["total_pymnt"]))]
# data["agg_col2"] = [1 if x % 2 == 0 else 0 for x in range(len(data["total_pymnt"]))]
# 
# data.groupby('agg_col1').sum()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# cudf_data["agg_col1"] = [1 if x % 2 == 0 else 0 for x in range(len(cudf_data["total_pymnt"]))]
# cudf_data["agg_col2"] = [1 if x % 2 == 0 else 0 for x in range(len(cudf_data["total_pymnt"]))]
# cudf_data.groupby('agg_col1').sum()

"""### **24.Transpose**

| Framework  | Pandas | cudf | 
| ---- | ---- | ---- | 
| Time | 1.1ms| 3.13ms|
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# sample = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})
# sample.transpose()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# sample_cudf = cudf.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})
# sample_cudf.transpose()

"""### **25.Time Series**

| Framework  | Pandas | cudf | daskcudf |
| ---- | ---- | ---- | ---- |
| Time | 213ms| 330ms| 79ms |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1.Pandas
# import datetime as dt
# 
# data["date_issued"] = pd.to_datetime(data["date_issued"])
# search_date = dt.datetime.strptime('2015-11-23', '%Y-%m-%d')
# display(data.query('date_issued <= @search_date'))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2.cudf
# import datetime as dt
# 
# cudf_data["date_issued"] = cudf.to_datetime(cudf_data["date_issued"])
# search_date = dt.datetime.strptime('2015-11-23', '%Y-%m-%d')
# display(cudf_data.query('date_issued <= @search_date'))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 3.daskcudf
# date_ddf = dask_cudf.from_cudf(cudf_data, npartitions=2)
# date_ddf.query('date_issued <= @search_date', local_dict={'search_date':search_date}).compute()

"""### **Conclusion**

* As per above example with this large scale [dataset](https://www.kaggle.com/c/home-credit-default-risk) cudf is working really faster on similar to pandas in somecases it beats pandas speed on operations.

### References

1. https://docs.rapids.ai/api/cudf/stable/10min.html
2. https://www.kaggle.com/c/home-credit-default-risk
"""